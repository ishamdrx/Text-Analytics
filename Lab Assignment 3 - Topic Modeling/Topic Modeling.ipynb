{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "692c6067-f555-4b00-b0f0-c26bc3f66cf1",
   "metadata": {},
   "source": [
    "**TOPIC MODELING**\n",
    "\n",
    "**Muhamed Hisham bin Mohamed Bahurudeen (IS01081947)**  \n",
    "**Muhammad Afiq Fikri Bin Ahmad Sabri (IS01082516)**\n",
    "\n",
    "*The LDA model achieved a coherence score of 0.561, indicating that the topics are reasonably interpretable.*  \n",
    "*This suggests that the model was able to uncover some meaningful themes within the dataset, though further refinement could enhance topic clarity.*" 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29aeda96-cab7-417b-adff-035666520a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\isham/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\isham/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\isham/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table with Articles and Topic:\n",
      "                                             Article  Topic\n",
      "0  wonder anyone could enlighten car saw day door...      3\n",
      "1  recently post article ask kind rate single mal...      0\n",
      "2  depend priority lot people put high priority g...      0\n",
      "3  excellent automatic find subaru legacy switch ...      3\n",
      "4  ford automobile need information whether ford ...      0\n",
      "\n",
      "Top Terms for Each Topic:\n",
      "Topic 0:\n",
      "- \"would\" (weight: 0.011)\n",
      "- \"one\" (weight: 0.008)\n",
      "- \"say\" (weight: 0.008)\n",
      "- \"people\" (weight: 0.008)\n",
      "- \"go\" (weight: 0.006)\n",
      "- \"think\" (weight: 0.006)\n",
      "- \"get\" (weight: 0.006)\n",
      "- \"know\" (weight: 0.006)\n",
      "- \"make\" (weight: 0.006)\n",
      "- \"u\" (weight: 0.005)\n",
      "\n",
      "Topic 1:\n",
      "- \"x\" (weight: 0.016)\n",
      "- \"key\" (weight: 0.013)\n",
      "- \"use\" (weight: 0.010)\n",
      "- \"encryption\" (weight: 0.009)\n",
      "- \"system\" (weight: 0.007)\n",
      "- \"information\" (weight: 0.006)\n",
      "- \"file\" (weight: 0.006)\n",
      "- \"privacy\" (weight: 0.005)\n",
      "- \"clipper\" (weight: 0.005)\n",
      "- \"security\" (weight: 0.005)\n",
      "\n",
      "Topic 2:\n",
      "- \"game\" (weight: 0.014)\n",
      "- \"team\" (weight: 0.012)\n",
      "- \"player\" (weight: 0.007)\n",
      "- \"play\" (weight: 0.007)\n",
      "- \"season\" (weight: 0.005)\n",
      "- \"league\" (weight: 0.005)\n",
      "- \"win\" (weight: 0.005)\n",
      "- \"year\" (weight: 0.004)\n",
      "- \"hockey\" (weight: 0.004)\n",
      "- \"score\" (weight: 0.004)\n",
      "\n",
      "Topic 3:\n",
      "- \"use\" (weight: 0.014)\n",
      "- \"get\" (weight: 0.008)\n",
      "- \"would\" (weight: 0.008)\n",
      "- \"one\" (weight: 0.007)\n",
      "- \"bit\" (weight: 0.006)\n",
      "- \"key\" (weight: 0.006)\n",
      "- \"know\" (weight: 0.006)\n",
      "- \"chip\" (weight: 0.006)\n",
      "- \"like\" (weight: 0.005)\n",
      "- \"window\" (weight: 0.005)\n",
      "\n",
      "Coherence Score: 0.561308335349498\n"
     ]
    }
   ],
   "source": [
    "# For text preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# For topic modeling\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "import pandas as pd\n",
    "\n",
    "# Download NLTK resources (run once)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the cleaned dataset\n",
    "df = pd.read_csv(\"Processed_News_Lemmatized.csv\")\n",
    "\n",
    "# Extract documents from th column\n",
    "documents = df['lemmatized'].dropna().tolist()\n",
    "\n",
    "# Preprocess function\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token.isalnum()]\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "# Preprocess all documents\n",
    "preprocessed_documents = [preprocess_text(doc) for doc in documents]\n",
    "\n",
    "# Create dictionary and corpus\n",
    "dictionary = corpora.Dictionary(preprocessed_documents)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in preprocessed_documents]\n",
    "\n",
    "# Train LDA model with 4 topics\n",
    "lda_model = LdaModel(corpus, num_topics=4, id2word=dictionary, passes=15)\n",
    "\n",
    "# Assign dominant topic to each document\n",
    "article_labels = []\n",
    "for doc in preprocessed_documents:\n",
    "    bow = dictionary.doc2bow(doc)\n",
    "    topics = lda_model.get_document_topics(bow)\n",
    "    dominant_topic = max(topics, key=lambda x: x[1])[0]\n",
    "    article_labels.append(dominant_topic)\n",
    "\n",
    "# Create DataFrame with results\n",
    "df_result = pd.DataFrame({\"Article\": documents, \"Topic\": article_labels})\n",
    "print(\"Table with Articles and Topic:\")\n",
    "print(df_result.head())\n",
    "\n",
    "# Show top terms for each topic\n",
    "print(\"\\nTop Terms for Each Topic:\")\n",
    "for idx, topic in lda_model.print_topics():\n",
    "    print(f\"Topic {idx}:\")\n",
    "    terms = [term.strip() for term in topic.split(\"+\")]\n",
    "    for term in terms:\n",
    "        weight, word = term.split(\"*\")\n",
    "        print(f\"- {word.strip()} (weight: {weight.strip()})\")\n",
    "    print()\n",
    "\n",
    "# Evaluate model using coherence score\n",
    "coherence_model = CoherenceModel(model=lda_model, texts=preprocessed_documents, dictionary=dictionary, coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(f\"Coherence Score: {coherence_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5e3470-d877-48f3-8005-ec53ffe6f26e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
